\chapter{Evaluation}
\label{chap:Evaluation}

In this chapter we'll go over different approaches and challenges for evaluating code completion, and what results we got from our experiments.

\section{Evaluation Overview}
Having code evalution is extremely important. One has to have clear benchmarks and measurement metrics, to be able to say exactly how good is the program, and whether it actually meets the expected result -- mainly, if there is indeed some improvement over the state-of-the-art implementation. If yes, the goal was achieved; if no -- you now know what parts you have to improve (for instance, speed, accuracy, or the general design of your system).

There are two types of evaluation you can do in software research: quantitative and qualitative.

In this case, quantitative, which can also be referred to as automatic, evaluation means creating metrics that can be evaluated without any additional resources or human participation. That means evaluating the accuracy of the model on some benchmarks, i.e. testing the model on test corpora and seeing how well it performs.

In qualitative evaluation, which is more appropriate for evaluating software tooling rather than statisctical models, researches essentially try to gauge the quality of the system by evaluating the human interaction experience. With something like code completion in mind, it could mean involving developers to use it, and record the impressions. In this case, the accuracy of the system would be based more on how it "feels" to use it, rather than what the numbers show.

\section{Challenges in Evaluating Completion}
For our use case both the quantitative and qualitative evaluations are fitting. The quantitative approach can be used for evaluating model accuracy itself, and qualitative can be utilised to test how the subsequent implementation feels and how exactly various sorting strategies influence the coding experience, and how well they "seem" to perform in comparison to previous implementations.

However, a proper qualitative evalution requires an extensive set-up, such as inolving multiple people, recording their coding process, conducting interviews, and so on. Unfortunately, we do not have the resources for that, and will be relying on the results of the quantitative evalution only.

(Remark: we will, however, record our own impressions we have from coding using different completion and sorting implementations.)

\section{The Experiment Itself}
The quantitative evaluation approach we decided to take is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

In particular, the idea with this approach is to recreate the coding process as it would happen naturally, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, our completion gets triggered for every token with the cursor between the 2nd and the 8th position (this range also taken from the \cite{Robb08a} paper). This means that starting with every second symbol of an alphabetic token, we invoke the completion and compare whether the result suggested at that step matches the one that followed in real history. This creates an illusion of it being a qualitative evalution setup, except with no human participation and instead with strict mathematical metrics.

In order for this to work, we have a dataset of methods from the Pharo IDE source code, and for each of them we repeat this process of calling the completion at each step and recording:
\begin{itemize}
    \item whether the correct completion was present among the suggestions at all
    \item if it was, what was its position
    \item additionally, how many times did we have the correct suggestion appearing within the first 10 resulting candidates
\end{itemize}

\section{Results}
demonstrate what we got as the result of evaluating and comparing different sorting strategies (different completion engines?)