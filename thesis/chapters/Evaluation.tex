\chapter{Evaluation}
\label{chap:Evaluation}

\epigraph{"It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong."}{--- \textup{Richard Feynman}}

In this chapter, we go over different approaches and challenges for evaluating code completion, and what results we got from our experiments.

\section{Evaluation Overview}
\label{sec:Evaluation-Overview}
Evaluation is an essential part of any research process. In the Richard Feynman quote above, the evaluation \textit{is} the experiment he refers to. To paraphrase and make it more relevant to software engineering, it does not matter what kind of tool you implemented and how well do you think it works, unless there are clear metrics the implementation had been measured on. Now, there are two main types of evaluation you can perform: quantitative and qualitative.

Quantitative, as can be guessed from its name, refers to the type of an evaluation experiment that is based on quantifiable data and uses objective metrics. This is the predominant evaluation approach in the natural sciences and computer science, i.e. measuring physical parameters in the case of the former, or testing performance score in the latter.

Qualitative methods, more native to fields such as sociology or psychology, are now being increasingly used in evaluation of computer systems and software tooling. The main objective is to get the "feel" for the system and deeper explore the human interaction experience, with the data gathered primarily from observations and interviews and subsequently analysed.

Conventionally, a proper qualitative evaluation requires an extensive set-up, involving multiple people, recording their coding process, conducting feedback interviews, and so on. However, due to lack of time, the qualitative evaluation I conducted for this project is more minimal. It involves manually selecting a number of common use cases and interacting with them as a user normally would, while recording the results and their meaning.

\section{Challenges in Evaluating Completion}
\label{sec:Evaluation-Challenges}
\oz{This section does not discuss the challenges of evaluating completion. There is no useful information about quantitative evaluation, and no information about completion and the complexity of telling if it's good or bad.}

\section{Quantitative Evaluation}
\label{sec:Evaluation-Quantitative}
\subsection{Experiment}
The quantitative evaluation approach \badstyle{we decided to take} is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

The idea with this approach is to recreate the coding process as it would happen naturally, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, \repetition{our completion gets triggered for every token with the cursor between the 2nd and the 8th position} (this range also taken from the \cite{Robb08a} paper). \repetition{Starting with every second symbol of an alphabetic token, we invoke the completion} and compare whether the result suggested at that step matches the one that followed in real history.

We used a dataset of methods from the Pharo IDE source code \insertion{collected by \cite{Zait20a}} \oz{you must put a reference, otherwise it's impossible for your readers to know what is this dataset, where did it come from, how many methods does it contain, etc.}, and for each of them we repeat this process of calling the completion at each step and performing the following:
\begin{itemize}
    \item check whether the correct completion was present in the list of suggestions
    \item if it was, we only consider the results successful if the correct completion was also present in the top-10 suggestions
    \item for those cases we record the current sequence (i.e. what has been "typed in" up to that point), prefix length, the actual correct match, and its position in the list of suggestions
    \item we also calculate the accuracy (average success rate) for all the methods evaluated (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results)
\end{itemize}

\subsection{Results}
Table~\ref{table:quan1} presents the accuracy for each of the classes and sorting strategies tested.

\oz{Explain what are the three models that you are comparing: unigram, bigram, and alphabetic. Why did you choose those three, why not 3-gram and 4-gram models? This is the first time when alphabetic sorting should be mentioned (as a "random baseline")}

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
    \hline
    \textbf{Class} & \textbf{\# of Methods} & \textbf{Alphabetic} & \textbf{Unigram} & \textbf{Bigram} \\ \hline
    OrderedCollection & 64 & 0.32 & 0.38 & 0.30 \\ 
    CompletionEvaluation & 9 & 0.24 & 0.25 & 0.22 \\ 
    FrequencyCompletionSorter & 2 & 0.48 & 0.58 & 0.47 \\ 
    NgramModel & 21 & 0.32 & 0.36 & 0.29 \\ \hline
    \end{tabular}
\caption{Quantitative evaluation, average success ratio}
\label{table:quan1}
\end{table}

\oz{And general rules of using tables: (1) Every table must be referenced in text, for example, "In Table~\ref{table:quan1} you can see..."; (2) The text around that reference must be sufficient to understand the table (describe every column, discuss the values); (3) Caption of every table must be sufficient to understand that table without searching for its reference in the main text (captions such as "Qualitative evaluation, example 1" are not enough). Same rules apply to all figures.}

We can see that the unigram sorter performs better than the alphabetic sorter, as expected. However, surprisingly the bigram sorter performs worse than the alphabetic sorter. \notclear{One reason might be the problems with training the model in regards to delimiters and other kinds of non-alphabetic tokens, which are plentiful in the code, and yet are difficult to tackle for correct training.} \oz{It's not clear what you mean.}

\section{Qualitative Evaluation}
\label{sec:Evaluation-Qualitative}
\subsection{Experiment}
Even though the hard numbers seem to show that the bigram strategy is not any good, it really does feel as if it is actually better than the alphabetic approach when used during a regular coding process. To demonstrate this, with no bias towards any of the sorting strategies, we preselected 10 cases for which we were interested to see completion suggestions and how accurate they were, and manually tested each of them. Below are the explanations and results for each of the 10 cases.

\subsection{Results}
Note: if the \textit{completion suggestion} and \textit{position} columns contain '-' for any sorting strategy, it means that in the first 10 suggestions we did not see the results we were expecting to see, based on the context at hand.

It should be taken into consideration, however, that all the results are only recorded after two symbols were typed in, and having correct suggestions with such a short prefix can be considered best-case scenario (as the correct suggestions would definitely be more likely to appear with a longer prefix). Ergo, the absence of correct suggestions at this point does not signal a complete failure of the completion engine or a sorting strategy used, but rather hints that there is more work required to get to the correct suggestions. It is unideal, and is exactly the reason for trying to improve the completion tool.

\begin{description}
\item [Example 1:] \hfill
\begin{lstlisting}
    col := OrderedCollection new.
    col ad
\end{lstlisting}
This was typed into the Playground and the results we were interested in were recorded with the cursor directly after \textit{ad}. For this context, the completion suggestions we were expecting as developers were \textit{add:} and, following, \textit{addAll:}. Here's what we got:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}add:\\ addAll:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 1}
\label{table:qual1}
\end{table}
Both alphabetic and bigram sorters didn't manage to provide any relevant results in the top 10 suggestions, whereas the result from the unigram was exactly what we wanted.

\item [Example 2:] \hfill
\begin{lstlisting}
    something := 1.
    (something = 1) if
\end{lstlisting}
Another example coded in the Playground, with the cursor at \textit{if}. The expected results would be \textit{ifTrue:}, \textit{ifTrue:ifFalse:}, and \textit{ifFalse:}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}ifTrue:\\ ifTrue:ifFalse:\\ ifFalse:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & ifTrue: & 5 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 2}
\label{table:qual2}
\end{table}
The alphabetic sorter once again did not suggest anything of interest right away, the unigram demonstrated an exemplary performance, and bigram, while less than ideal, still suggested one of the correct options in the top 10 position (albeit a bit lower, which would require either typing more symbols or pressing the down arrow key).

\item [Example 3:] \hfill
\begin{lstlisting}
    TokenProcessing >> returnProcessedData
        | tokensDataFrame |
        tokensDataFrame := self re
\end{lstlisting}
This was typed in the Editor, in the method \textit{returnProcessedData} of class \textit{TokenProcessing}. This is done to try out some more elaborate examples where the local context can play a role. Here, as \textit{rejectInvalidTokens}, \textit{replaceWithPlaceholders} and \textit{readFile} are also methods on the instance side of the \textit{TokenProcessing} class, we expect to get their names as suggestions after typing \textit{self}. Particularly, as the cursor is positioned after \textit{re}, we expect to get only these 3 methods, as they are the ones starting with \textit{re}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & readFile & 3 \\ \hline
    Unigram & - & - \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}rejectInvalidTokens\\ replaceWithPlaceholders\\ readFile\end{tabular} & \begin{tabular}[c]{@{}r@{}}7\\ 8\\ 9\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 3}
\label{table:qual3}
\end{table}
It seems that here the unigram approach fell short, whereas the bigram approach, trained on prioriting local method name after \textit{self}, has performed quite well. In the alphabetic approach, the other two method suggestions seem to be buried below the first 10, whereas \textit{readFile} gets a better position.

\item [Example 4:] \hfill
\begin{lstlisting}
    TokenProcessing >> replaceWithPlaceholders
        | data tokens tokenTypes arr |
        data := self returnProcessedData.
        to
\end{lstlisting}
Typing \textit{to} we are expecting to have \textit{tokens} and \textit{tokenTypes}, as those are temporary variables we have declared and we might want to initialise them with some value. Here is what we got suggested:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}tokenTypes\\ tokens\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 4}
\label{table:qual4}
\end{table}
The order in the unigram and bigram cases is a bit better, as having a shorter word suggested first is preferable, but in general all three methods worked well in this one.

\item [Example 5:] \hfill
\begin{lstlisting}
    UnigramTableCreator ne
\end{lstlisting}
Typing this in the Playground, we expect \textit{new} and maybe even \textit{new:}, depending on the context.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}new:\\ new\end{tabular} & \begin{tabular}[c]{@{}r@{}}4\\ 8\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 5}
\label{table:qual5}
\end{table}
The best positions are for the unigram sorter, but in general all the sorters suggested the desired options in the top 10.

\item [Example 6:] \hfill
\begin{lstlisting}
    in
\end{lstlisting}
In the Pharo IDE it is also possible to get completion suggestions even when creating a new method, and in this case we went to the instance side of a class and started typing \textit{in} as method name, wanting to get \textit{initialize}, as it is a very common method one might want to create in a class.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & initialize & 2 \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 6}
\label{table:qual6}
\end{table}
The unigram sorter is the only that was able to suggest \textit{initialize} from only 2 characters typed in, and at the 2 position!

\item [Example 7:] \hfill
\begin{lstlisting}
    ExperimentalNgramTest >> testMostLikelyContinuations
    | expected actual word |
    word := 'lorem'.
    actual := model mostLikelyContinuations: word asNgram top: 3.
    expected := #('ipsum' 'dolor' 'lorem').
    self as
\end{lstlisting}
In test methods having an \textit{assert:equals:} call is quite common, and as there are a lot of tests being written, we are interested in having a completion suggestion that is as accurate as possible after \textit{as}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}assert:\\ assert:equals:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & assert:equals: & 1 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 7}
\label{table:qual7}
\end{table}
Both the unigram and bigram sorters showed good results.

\item [Example 8:] \hfill
\begin{lstlisting}
    CompletionEvaluation >> averageSuccessRatio
    | sumS sumA |
	sumS := 0.
	sumA := 0.
	countSuccess do: [ :each | su ]
\end{lstlisting}
Here we are interested in completion inside the block, so our cursor is after \textit{su} before the bracket \textit{]}. Since inside the block we want to do assignment for the temporary variables, we are expecting \textit{sumA} and \textit{sumS} as suggestions.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}sumA\\ sumS\\ super\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 8}
\label{table:qual8}
\end{table}

\item [Example 9:] \hfill
\begin{lstlisting}
    RBScanner sc
\end{lstlisting}
In the Playground we want to get suggested method names on the class side of \textit{RBScanner} that begin with \textit{sc}. There are two: \textit{scanTokens:} and \textit{scanTokenObjects:}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}scanTokens:\\ scanTokenObjects:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 9}
\label{table:qual9}
\end{table}
On the one hand, all sorters showed essentially the same result; however, on the other hand, \textit{scanTokens:} is more likely to be used next to \textit{RBscanner} and it is a shorter token, so the bigram result is slightly superior.

\item [Example 10:] \hfill
\begin{lstlisting}
    UnigramTableCreator >> writeToFile
    | data stream |
    data := self frequencies.
    stream := TokenProcessing frequenciesFile writeStream.
    (NeoCSVWriter on: stream)
        nextPut: #(key value);
        ne
\end{lstlisting}
Now, for an even more creative example, in Pharo there can be multiple messages sent to a receiver, as long as they are all separated by a semicolon \textit{;}. Hence, having a cursor after \textit{ne}, we know we are sending a message to \textit{NeoCSVWriter on: stream} and want to get suitable suggestions, which are \textit{nextPut:} and \textit{nextPutAll:}.

In the table of results below we see that the unigram sorter got the best result.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}nextPutAll:\\ nextPut:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 10}
\label{table:qual10}
\end{table}
\end{description}

To briefly sum up the results of the manual qualitative evaluation, the unigram sorter is still the best one out of the three, the same as in the quantitative evaluation. However, as seen in these examples, the bigram sorter is not very far behind, also more often than not prioriting the desired completions, which disagrees with the quantitative evaluation. So, while based on cold data, the bigram sorter is the worst of the three \textit{on average}, from the manual evaluation it seems that it gives perfectly adequate results even for more 'tricky' examples.

\section{Summary}
\label{sec:Evaluation-Summary}
\begin{itemize}
    \item Quantitative (automatic) evaluation involves evaluating the model on \badstyle{some benchmarks}, whereas the qualitative (manual) approach involves evaluating the results \badstyle{on the go}, as the program is being normally executed.
    \item Based on the evaluation tool that we created and used to evaluate average success rate (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results) for each sorting strategy, the unigram sorter performed better than the alphabetic sorter but, surprisingly, the bigram one performed worse.
    \item For the qualitative evaluation, we picked 10 interesting completion cases (with no bias towards any strategy) and tested them in the real environment and recorded the positions of completion suggestions we were interested in. Based on this, the unigram approach was once again the best, but this time the bigram approach \badstyle{followed not far behind}, and the alphabetic was worse.
    \item \remove{The unigram sorter is a clear improvement on the alphabetic approach, whereas the bigram sorter seems to lack the accuracy.} \oz{Move this to the Conclusion. But don't say "clear improvement". How do you define "clear"?}
\end{itemize}