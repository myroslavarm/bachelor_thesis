\chapter{Evaluation}
\label{chap:Evaluation}

In this chapter we'll go over different approaches and challenges for evaluating code completion, and what results we got from our experiments.

\section{Evaluation Overview}
\label{sec:Evaluation-Overview}
Evaluating results of an experiment is extremely important. To quote Richard Feynman, "It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong". In computer science, one has to have clear benchmarks and measurement metrics, to be able to say whether the results were satisfactory and the initial hypothesis was confirmed. The goal is to improve the existing implementation, whether it's to come up with an algorithm that solves the problem faster, or gives more precise results. In evaluation, both success and failure are important: success means you achieved the goal, but the failure to improve a system just means more work might be needed and one is closer to the goal than previously. Therefore, whatever the results, any experiment outcome is valuable, and evaluation is an essential part of the research process.

There are two types of evaluation you can do in software research: quantitative and qualitative.

In this case, quantitative, which can also be referred to as automatic, evaluation means creating metrics that can be evaluated without any additional resources or human participation. That means evaluating the accuracy of the model on some benchmarks, i.e. testing the model on test corpora and seeing how well it performs.

In qualitative evaluation, which is more appropriate for evaluating software tooling rather than statisctical models, researches essentially try to gauge the quality of the system by evaluating the human interaction experience. With something like code completion in mind, it could mean involving developers to use it, and record the impressions. In this case, the accuracy of the system would be based more on how it "feels" to use it, rather than what the numbers show.

\section{Challenges in Evaluating Completion}
\label{sec:Evaluation-Challenges}
For our use case both the quantitative and qualitative evaluations are fitting. The quantitative approach can be used for evaluating model accuracy itself, and qualitative can be utilised to test how the subsequent implementation feels and how exactly various sorting strategies influence the coding experience, and how well they "seem" to perform in comparison to previous implementations.

Normally, a proper qualitative evalution requires an extensive set-up, such as inolving multiple people, recording their coding process, conducting interviews, and so on. Unfortunately, we do not have the resources for that, and will be conducting a more minimalist version of qualitative evaluation: in particular, we will manually test a selected number of cases and compare the results across various sorting strategies ourselves.

\section{Quantitative Evaluation}
\label{sec:Evaluation-Quantitative}
\subsection{Experiment}
The quantitative evaluation approach we decided to take is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

In particular, the idea with this approach is to recreate the coding process as it would happen naturally, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, our completion gets triggered for every token with the cursor between the 2nd and the 8th position (this range also taken from the \cite{Robb08a} paper). This means that starting with every second symbol of an alphabetic token, we invoke the completion and compare whether the result suggested at that step matches the one that followed in real history. This creates an illusion of it being a qualitative evalution setup, except with no human participation and instead with strict mathematical metrics.

In order for this to work, we have a dataset of methods from the Pharo IDE source code, and for each of them we repeat this process of calling the completion at each step and performing the following:
\begin{itemize}
    \item check whether the correct completion was present among the suggestions at all
    \item if it was, we only consider the results successful if the correct completion was also present in the top-10 suggestions
    \item for those cases we record the current sequence (i.e. what has been "typed in" up to that point), prefix length, the actual correct match, and its position in the list of suggestions
    \item we also calculate an average success rate for all the methods evaluated (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results)
\end{itemize}

\subsection{Results}
demonstrate what we got as the result of evaluating and comparing different sorting strategies
\begin{itemize}
    \item alphabetic/frequency/bigram sorters for OrderedCollection methods results (avg success rate)
    \item alphabetic/frequency/bigram sorters for one of the implementation classes (avg success rate)
    \item the idea being, one class on which the model was trained, and one on which it was not?
\end{itemize}

\section{Qualitative Evaluation}
\label{sec:Evaluation-Qualitative}
\subsection{Experiment}
describe the qualitative setup
\begin{itemize}
    \item essentially brainstorm 10 cases where completion is lacking
    \item manually test those 10 cases on alphebtic/frequency/bigram and record the results
\end{itemize}
\subsection{Results}
describe the results of the qualitative setup

\section{Summary}
\label{sec:Evaluation-Summary}
Briefly sum up the results of the experiment and whether the problem is "solved".