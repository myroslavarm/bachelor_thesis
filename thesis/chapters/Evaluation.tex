\chapter{Evaluation}
\label{chap:Evaluation}

\epigraph{"It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong."}{--- \textup{Richard Feynman}}

In this chapter, I go over different approaches and challenges for evaluating code completion, and what results I got from the experiments.

\section{Evaluation Overview}
\label{sec:Evaluation-Overview}
Evaluation is an essential part of any research process. In the Richard Feynman quote above, the evaluation \textit{is} the experiment he is referring to. To paraphrase and make it more relevant to software engineering, it does not matter what kind of tool you implemented and how well do you think it works, unless there are clear metrics the implementation had been measured on and you actually have the data to back up your argument with. Now, there are two main types of evaluation one can perform: quantitative and qualitative.

Quantitative, as can be guessed from its name, refers to the type of an evaluation experiment that is based on quantifiable data and uses objective metrics. This is the predominant evaluation approach in the natural sciences and computer science, i.e. measuring physical parameters in the case of the former, or testing performance scores in the latter.

Qualitative methods, more native to fields such as sociology or psychology, are now being increasingly used in evaluation of computer systems and software tooling. The main objective is to get the "feel" for the system and deeper explore the human interaction experience, with the data gathered primarily from observations and interviews and subsequently analysed. (\cite{Hazz06a} in their paper discuss the qualitative approach and its relationship to the quantitative one.)

Conventionally, a proper qualitative evaluation requires an extensive set-up, involving multiple people, recording their coding process, conducting feedback interviews, and so on. However, due to lack of time, the qualitative evaluation I conducted for this project is more minimal. It involves manually selecting a number of common use cases and interacting with them as a user normally would, while recording the results and their meaning. (\cite{Kapl05a} describe an extensive qualitative evaluation example and talk about the key benefits and considerations of this approach.)

\section{Challenges in Evaluating Completion}
\label{sec:Evaluation-Challenges}
Coming up with a way to most accurately evaluate code completion is not trivial. One needs to have an idea how best to reproduce the process and what the metrics should be, as well as have an access to the resources needed.

As per the qualitative approach, one can devise a user study, where developers would use the tool as is, and the experiment will be recorded. As has been mentioned already, the obvious drawbacks of this approach are that it is difficult to set it up due to resource constraints (time, money, etc.).

Following the quantitative approach, code completion evaluation can be done via means of benchmarking, i.e. automatically evaluating the performance on test data. In this case, there are a few data-related constraints. One will have to prepare a dataset if it is not already available, and gathering enough test data can be tricky. On the other hand, the test dataset can be just too big, requiring either a monetary investment into more GPUs or a time investment into developing a complex evaluation tool, which will both deal with a large quantity of data, and accurately reproduce the process of completing code.

Finally, at the end of the evaluation the goal is to be able to say whether the completion performs well or badly, and just \textit{how} well or \textit{how} badly. So another challenge, which applies to both the qualitative and the quantitative evaluation, comes in devising suitable metrics.

\section{Quantitative Evaluation}
\label{sec:Evaluation-Quantitative}
\subsection{Experiment}
The quantitative evaluation approach I performed is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

The idea with this approach is to recreate the coding process as it would happen naturally, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, I trigger the completion for every token with the cursor between the 2nd and the 8th position (this range also taken from the \cite{Robb08a} paper) and compare whether the result suggested at that step matches the one that followed in real history.

For evaluation I took several classes at random, each with many methods, implemented in Pharo. For each of them I repeat this process of calling the completion at each step and perform the following:
\begin{itemize}
    \item check whether the correct completion was present in the list of suggestions
    \item if it was, I only consider the results successful if the correct completion was also present in the top-10 suggestions
    \item for those cases I record the current sequence (i.e. what has been "typed in" up to that point), prefix length, the actual correct match, and its position in the list of suggestions
    \item I also calculate the accuracy (average success rate) for all the methods evaluated (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results)
\end{itemize}

\subsection{Results}
Table~\ref{table:quan1} presents the accuracy for each of the classes and sorting strategies tested.

\oz{Explain what are the three models that you are comparing: unigram, bigram, and alphabetic. Why did you choose those three, why not 3-gram and 4-gram models? This is the first time when alphabetic sorting should be mentioned (as a "random baseline")}

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
    \hline
    \textbf{Class} & \textbf{\# of Methods} & \textbf{Alphabetic} & \textbf{Unigram} & \textbf{Bigram} \\ \hline
    OrderedCollection & 64 & 0.32 & 0.38 & 0.30 \\ 
    CompletionEvaluation & 9 & 0.24 & 0.25 & 0.22 \\ 
    FrequencyCompletionSorter & 2 & 0.48 & 0.58 & 0.47 \\ 
    NgramModel & 21 & 0.32 & 0.36 & 0.29 \\ \hline
    \end{tabular}
\caption{Quantitative evaluation: the average success ratio (accuracy) for each class and sorting strategy}
\label{table:quan1}
\end{table}

\oz{And general rules of using tables: (1) Every table must be referenced in text, for example, "In Table~\ref{table:quan1} you can see..."; (2) The text around that reference must be sufficient to understand the table (describe every column, discuss the values); (3) Caption of every table must be sufficient to understand that table without searching for its reference in the main text (captions such as "Qualitative evaluation, example 1" are not enough). Same rules apply to all figures.}

From the results, we can see that the unigram sorter performs better than the alphabetic sorter, as expected. However, surprisingly the bigram sorter performs worse than the alphabetic sorter. \notclear{One reason might be the problems with training the model in regards to delimiters and other kinds of non-alphabetic tokens, which are plentiful in the code, and yet are difficult to tackle for correct training.} \oz{It's not clear what you mean.}

\section{Qualitative Evaluation}
\label{sec:Evaluation-Qualitative}
\subsection{Experiment}
Even though the quantitative experiment results seem to show that the bigram strategy is not any good, it really does feel as if it is actually better than the alphabetic approach when used during a regular coding process. To demonstrate this, with no bias towards any of the sorting strategies, I preselected 10 cases for which we were interested to see completion suggestions and how accurate they were, and manually tested each of them. Below are the explanations and results for each of the 10 cases.

\subsection{Results}
Note: if the \textit{completion suggestion} and \textit{position} columns contain '-' for any sorting strategy, it means that the expected results were not present in the top 10 suggestions.

It should be taken into consideration, however, that all the results are only recorded after two symbols were typed in, and having correct suggestions with such a short prefix can be considered best-case scenario (as the correct suggestions would definitely be more likely to appear with a longer prefix). Ergo, the absence of correct suggestions at this point does not signal a complete failure of the completion engine or a sorting strategy used, but rather hints that there is more work required to get to the correct suggestions. It is unideal, and is exactly the reason for trying to improve the completion tool.

\begin{description}
\item [Example 1:] \hfill
\begin{lstlisting}
    col := OrderedCollection new.
    col ad
\end{lstlisting}
This was typed into the Playground and the results I was interested in were recorded with the cursor directly after \textit{ad}. For this context, the completion suggestions the developers would expect were \textit{add:} and, following, \textit{addAll:}. Here's what I got:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}add:\\ addAll:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 1}
\label{table:qual1}
\end{table}
Both alphabetic and bigram sorters didn't manage to provide any relevant results in the top 10 suggestions, whereas the result from the unigram was exactly what I was expecting.

\item [Example 2:] \hfill
\begin{lstlisting}
    something := 1.
    (something = 1) if
\end{lstlisting}
Another example coded in the Playground, with the cursor at \textit{if}. The expected results would be \textit{ifTrue:}, \textit{ifTrue:ifFalse:}, and \textit{ifFalse:}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}ifTrue:\\ ifTrue:ifFalse:\\ ifFalse:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & ifTrue: & 5 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 2}
\label{table:qual2}
\end{table}
The alphabetic sorter once again did not suggest anything of interest right away, the unigram demonstrated an exemplary performance, and bigram, while less than ideal, still suggested one of the correct options in the top 10 position (albeit a bit lower, which would require either typing more symbols or pressing the down arrow key).

\item [Example 3:] \hfill
\begin{lstlisting}
    TokenProcessing >> returnProcessedData
        | tokensDataFrame |
        tokensDataFrame := self re
\end{lstlisting}
This was typed in the Editor, in the method \textit{returnProcessedData} of class \textit{TokenProcessing}. This is done to try out some more elaborate examples where the local context can play a role. Here, as \textit{rejectInvalidTokens}, \textit{replaceWithPlaceholders} and \textit{readFile} are also methods on the instance side of the \textit{TokenProcessing} class, I expect to get their names as suggestions after typing \textit{self}. Particularly, as the cursor is positioned after \textit{re}, I expect to get only these 3 methods, as they are the ones starting with \textit{re}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & readFile & 3 \\ \hline
    Unigram & - & - \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}rejectInvalidTokens\\ replaceWithPlaceholders\\ readFile\end{tabular} & \begin{tabular}[c]{@{}r@{}}7\\ 8\\ 9\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 3}
\label{table:qual3}
\end{table}
It seems that here the unigram approach fell short, whereas the bigram approach, trained on prioriting local method name after \textit{self}, has performed quite well. In the alphabetic approach, the other two method suggestions seem to be buried below the first 10, whereas \textit{readFile} gets a better position.

\item [Example 4:] \hfill
\begin{lstlisting}
    TokenProcessing >> replaceWithPlaceholders
        | data tokens tokenTypes arr |
        data := self returnProcessedData.
        to
\end{lstlisting}
Typing \textit{to} I am expecting to have \textit{tokens} and \textit{tokenTypes}, as those are temporary variables that were declared and a developer might want to initialise them with some value. Here is what the suggestions were:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}tokenTypes\\ tokens\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 4}
\label{table:qual4}
\end{table}
The order in the unigram and bigram cases is a bit better, as having a shorter word suggested first is preferable, but in general all three methods worked well in this one.

\item [Example 5:] \hfill
\begin{lstlisting}
    UnigramTableCreator ne
\end{lstlisting}
Typing this in the Playground, I expect \textit{new} and maybe even \textit{new:}, depending on the context.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}new:\\ new\end{tabular} & \begin{tabular}[c]{@{}r@{}}4\\ 8\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 5}
\label{table:qual5}
\end{table}
The best positions are for the unigram sorter, but in general all the sorters suggested the desired options in the top 10.

\item [Example 6:] \hfill
\begin{lstlisting}
    in
\end{lstlisting}
In the Pharo IDE it is also possible to get completion suggestions even when creating a new method, and in this case I went to the instance side of a class and started typing \textit{in} as method name, wanting to get \textit{initialize}, as it is a very common method one might want to create in a class.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & initialize & 2 \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 6}
\label{table:qual6}
\end{table}
The unigram sorter is the only that was able to suggest \textit{initialize} from only two characters typed in, and at the 2nd position!

\item [Example 7:] \hfill
\begin{lstlisting}
    ExperimentalNgramTest >> testMostLikelyContinuations
    | expected actual word |
    word := 'lorem'.
    actual := model mostLikelyContinuations: word asNgram top: 3.
    expected := #('ipsum' 'dolor' 'lorem').
    self as
\end{lstlisting}
In test methods having an \textit{assert:equals:} call is quite common, and as there are a lot of tests being written, I am interested in having a completion suggestion that is as accurate as possible after \textit{as}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}assert:\\ assert:equals:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & assert:equals: & 1 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 7}
\label{table:qual7}
\end{table}
Both the unigram and bigram sorters showed good results.

\item [Example 8:] \hfill
\begin{lstlisting}
    CompletionEvaluation >> averageSuccessRatio
    | sumS sumA |
	sumS := 0.
	sumA := 0.
	countSuccess do: [ :each | su ]
\end{lstlisting}
Here I am interested in completion inside the block, so the cursor is after \textit{su} before the bracket \textit{]}. Since inside the block I want to do assignment for the temporary variables, I am expecting \textit{sumA} and \textit{sumS} as suggestions.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}sumA\\ sumS\\ super\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 8}
\label{table:qual8}
\end{table}

\item [Example 9:] \hfill
\begin{lstlisting}
    RBScanner sc
\end{lstlisting}
In the Playground I want to get suggested method names on the class side of \textit{RBScanner} that begin with \textit{sc}. There are two: \textit{scanTokens:} and \textit{scanTokenObjects:}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}scanTokens:\\ scanTokenObjects:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 9}
\label{table:qual9}
\end{table}
On the one hand, all sorters showed essentially the same result; however, on the other hand, \textit{scanTokens:} is more likely to be used next to \textit{RBscanner} and it is a shorter token, so the bigram result is slightly superior.

\item [Example 10:] \hfill
\begin{lstlisting}
    UnigramTableCreator >> writeToFile
    | data stream |
    data := self frequencies.
    stream := TokenProcessing frequenciesFile writeStream.
    (NeoCSVWriter on: stream)
        nextPut: #(key value);
        ne
\end{lstlisting}
Now, for an even more creative example, in Pharo there can be multiple messages sent to a receiver, as long as they are all separated by a semicolon \textit{;}. Hence, having a cursor after \textit{ne}, I know I am sending a message to \textit{NeoCSVWriter on: stream} and want to get suitable suggestions, which are \textit{nextPut:} and \textit{nextPutAll:}.

In the table of results below we see that the unigram sorter got the best result.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}nextPutAll:\\ nextPut:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 10}
\label{table:qual10}
\end{table}
\end{description}

To briefly sum up the results of the manual qualitative evaluation, the unigram sorter is still the best one out of the three, the same as in the quantitative evaluation. However, as seen in these examples, the bigram sorter is not very far behind, also more often than not prioriting the desired completions, which disagrees with the quantitative evaluation. So, while based on cold data, the bigram sorter is the worst of the three \textit{on average}, from the manual evaluation it seems that it gives perfectly adequate results even for more 'tricky' examples.

\section{Summary}
\label{sec:Evaluation-Summary}
\begin{itemize}
    \item Quantitative evaluation involves benchmarking the model and calculating performance scores, whereas the qualitative approach involves conducting the evaluation via setting up "live" human studies, recording feedback interviews, and so on.
    \item Based on the evaluation tool that I created and used to evaluate average success rate (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results) for each sorting strategy, the unigram sorter performed better than the alphabetic sorter but, surprisingly, the bigram one performed worse.
    \item For the qualitative evaluation, I picked 10 interesting completion cases (with no bias towards any strategy) and tested them in the real environment and recorded the positions of the desired completion suggestions. Based on this, the unigram approach was once again the best, but this time the bigram approach \badstyle{followed not far behind}, and the alphabetic was worse.
    \item \remove{The unigram sorter is a clear improvement on the alphabetic approach, whereas the bigram sorter seems to lack the accuracy.} \oz{Move this to the Conclusion. But don't say "clear improvement". How do you define "clear"?}
\end{itemize}