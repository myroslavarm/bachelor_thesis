\chapter{Evaluation}
\label{chap:Evaluation}

In this chapter we'll go over different approaches and challenges for evaluating code completion, and what results we got from our experiments.

\section{Evaluation Overview}
Having code evalution is extremely important. One has to have clear benchmarks and measurement metrics, to be able to say exactly how good is the program, and whether it actually meets the expected result -- mainly, if there is indeed some improvement over the state-of-the-art implementation. If yes, the goal was achieved; if no -- you now know what parts you have to improve (for instance, speed, accuracy, or the general design of your system).

There are two types of evaluation you can do in software research: quantitative and qualitative.

In this case, quantitative, which can also be referred to as automatic, evaluation means creating metrics that can be evaluated without any additional resources or human participation. That means evaluating the accuracy of the model on some benchmarks, i.e. testing the model on test corpora and seeing how well it performs.

In qualitative evaluation, which is more appropriate for evaluating software tooling rather than statisctical models, researches essentially try to gauge the quality of the system by evaluating the human interaction experience. With something like code completion in mind, it could mean involving developers to use it, and record the impressions. In this case, the accuracy of the system would be based more on how it "feels" to use it, rather than what the numbers show.

\section{Challenges in Evaluating Completion}
For our use case both the quantitative and qualitative evaluations are fitting. The quantitative approach can be used for evaluating model accuracy itself, and qualitative can be utilised to test how the subsequent implementation feels and how exactly various sorting strategies influence the coding experience, and how well they "seem" to perform in comparison to previous implementations.

However, a proper qualitative evalution requires an extensive set-up, such as inolving multiple people, recording their coding process, conducting interviews, and so on. Unfortunately, we do not have the resources for that, and will be relying on the results of the quantitative evalution only.

(Remark: we will, however, record our own impressions we have from coding using different completion and sorting implementations.)

\section{The Experiment Itself}
describing what we did and how we set it up exactly

\section{Results}
demonstrate what we got as the result of evaluating and comparing different sorting strategies