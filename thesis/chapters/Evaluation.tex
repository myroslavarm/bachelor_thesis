\chapter{Evaluation}
\label{chap:Evaluation}

In this chapter we'll go over different approaches and challenges for evaluating code completion, and what results we got from our experiments.

\section{Evaluation Overview}
\label{sec:Evaluation-Overview}
Evaluating results of an experiment is extremely important. To quote Richard Feynman, "It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong". In computer science, one has to have clear benchmarks and measurement metrics, to be able to say whether the results were satisfactory and the initial hypothesis was confirmed. The goal is to improve the existing implementation, whether it's to come up with an algorithm that solves the problem faster, or gives more precise results. In evaluation, both success and failure are important: success means you achieved the goal, but the failure to improve a system just means more work might be needed and one is closer to the goal than previously. Therefore, whatever the results, any experiment outcome is valuable, and evaluation is an essential part of the research process.

There are two types of evaluation you can do in software research: quantitative and qualitative.

In this case, quantitative, which can also be referred to as automatic, evaluation means creating metrics that can be evaluated without any additional resources or human participation. That means evaluating the accuracy of the model on some benchmarks, i.e. testing the model on test corpora and seeing how well it performs.

In qualitative evaluation, which is more appropriate for evaluating software tooling rather than statisctical models, researches essentially try to gauge the quality of the system by evaluating the human interaction experience. With something like code completion in mind, it could mean involving developers to use it, and record the impressions. In this case, the accuracy of the system would be based more on how it "feels" to use it, rather than what the numbers show.

\section{Challenges in Evaluating Completion}
\label{sec:Evaluation-Challenges}
For our use case both the quantitative and qualitative evaluations are fitting. The quantitative approach can be used for evaluating model accuracy itself, and qualitative can be utilised to test how the subsequent implementation feels and how exactly various sorting strategies influence the coding experience, and how well they "seem" to perform in comparison to previous implementations.

Normally, a proper qualitative evalution requires an extensive set-up, such as inolving multiple people, recording their coding process, conducting interviews, and so on. Unfortunately, we do not have the resources for that, and will be conducting a more minimalist version of qualitative evaluation: in particular, we will manually test a selected number of cases and compare the results across various sorting strategies ourselves.

\section{Quantitative Evaluation}
\label{sec:Evaluation-Quantitative}
\subsection{Experiment}
The quantitative evaluation approach we decided to take is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

In particular, the idea with this approach is to recreate the coding process as it would happen naturally, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, our completion gets triggered for every token with the cursor between the 2nd and the 8th position (this range also taken from the \cite{Robb08a} paper). This means that starting with every second symbol of an alphabetic token, we invoke the completion and compare whether the result suggested at that step matches the one that followed in real history. This creates an illusion of it being a qualitative evalution setup, except with no human participation and instead with strict mathematical metrics.

In order for this to work, we have a dataset of methods from the Pharo IDE source code, and for each of them we repeat this process of calling the completion at each step and performing the following:
\begin{itemize}
    \item check whether the correct completion was present among the suggestions at all
    \item if it was, we only consider the results successful if the correct completion was also present in the top-10 suggestions
    \item for those cases we record the current sequence (i.e. what has been "typed in" up to that point), prefix length, the actual correct match, and its position in the list of suggestions
    \item we also calculate an average success rate for all the methods evaluated (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results)
\end{itemize}

\subsection{Results}
Now, here are the average success rates for each of the classes and sorting strategies we've tested. \textit{OrderedCollection} has 64 methods, \textit{CompletionEvaluation} has 9, \textit{FrequencyCompletionSorter} has just 2, and \textit{NgramModel} 21.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{CLASS} & \textbf{ALPHABETIC} & \textbf{UNIGRAM} & \textbf{BIGRAM} \\ \hline
    OrderedCollection & 0.31859756097560976 & 0.3827743902439024 & 0.29771341463414636 \\ \hline
    CompletionEvaluation & 0.24436363636363637 & 0.24581818181818182 & 0.22181818181818183 \\ \hline
    FrequencyCompletionSorter & 0.48314606741573035 & 0.5842696629213483 & 0.47191011235955055 \\ \hline
    NgramModel & 0.3246268656716418 & 0.36268656716417913 & 0.291044776119403 \\ \hline
    \end{tabular}
\caption{Quantitative evaluation results: average success ratio for each class and sorting strategy}
\label{table:quan1}
\end{table}

From this, we can see that on average the unigram sorter performs much better than the alphabetic sorter, as expected. However, surprisingly the bigram sorter not only does not perform better than the unigram strategy, it also performs just a little worse than the alphabetic sorter, which was not expected at all. One reason might be the problems with training the model in regards to delimiters and other kinds of non-alphabetic tokens, which are plentiful in the code, and yet are difficult to tackle for correct training.

\section{Qualitative Evaluation}
\label{sec:Evaluation-Qualitative}
\subsection{Experiment}
Even though the hard numbers seem to show that the bigram strategy is not any good, it really does feel as if it is actually better than the alphabetic approach when used during a regular coding process. To demonstrate this, with no bias towards any of the sorting strategies, we preselected 10 cases for which we were interested to see completion suggestions and how accurate they were, and manually tested each of them. Below are the explanations and results for each of the 10 cases.

\subsection{Results}
Note: if the \textit{completion suggestion} and \textit{position} columns contain '-' for any sorting strategy, it means that in the first 10 suggestions we did not see the results we were expecting to see, based on the context at hand.

It should be taken into consideration, however, that all the results are only recorded after two symbols were typed in, and having correct suggestions with such a short prefix can be considered best-case scenario (as the correct suggestions would definitely be more likely to appear with a longer prefix). Ergo, the absence of correct suggestions at this point does not signal a complete failure of the completion engine or a sorting strategy used, but rather hints that there is more work required to get to the correct suggestions. It is unideal, and is exactly the reason for trying to improve the completion tool.

\begin{description}
\item [Example 1:] \hfill
\begin{lstlisting}
    col := OrderedCollection new.
    col ad
\end{lstlisting}
This was typed into the Playground and the results we were interested in were recorded with the cursor directly after \textit{ad}. For this context, the completion suggestions we were expecting as developers were \textit{add:} and, following, \textit{addAll:}. Here's what we got:
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & - & - \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}add:\\ addAll:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 1}
\label{table:qual1}
\end{table}
Both alphabetic and bigram sorters didn't manage to provide any relevant results in the top 10 suggestions, whereas the result from the unigram was exactly what we wanted.

\item [Example 2:] \hfill
\begin{lstlisting}
    something := 1.
    (something = 1) if
\end{lstlisting}
Another example coded in the Playground, with the cursor at \textit{if}. The expected results would be \textit{ifTrue:}, \textit{ifTrue:ifFalse:}, and \textit{ifFalse:}.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & - & - \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}ifTrue:\\ ifTrue:ifFalse:\\ ifFalse:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    bigram & ifTrue: & 5 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 2}
\label{table:qual2}
\end{table}
The alphabetic sorter once again did not suggest anything of interest right away, the unigram demonstrated an exemplary performance, and bigram, while less than ideal, still suggested one of the correct options in the top 10 position (albeit a bit lower, which would require either typing more symbols or pressing the down arrow key).

\item [Example 3:] \hfill
\begin{lstlisting}
    TokenProcessing >> returnProcessedData
        | tokensDataFrame |
        tokensDataFrame := self re
\end{lstlisting}
This was typed in the Editor, in the method \textit{returnProcessedData} of class \textit{TokenProcessing}. This is done to try out some more elaborate examples where the local context can play a role. Here, as \textit{rejectInvalidTokens}, \textit{replaceWithPlaceholders} and \textit{readFile} are also methods on the instance side of the \textit{TokenProcessing} class, we expect to get their names as suggestions after typing \textit{self}. Particularly, as the cursor is positioned after \textit{re}, we expect to get only these 3 methods, as they are the ones starting with \textit{re}.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & readFile & 3 \\ \hline
    unigram & - & - \\ \hline
    bigram & \begin{tabular}[c]{@{}l@{}}rejectInvalidTokens\\ replaceWithPlaceholders\\ readFile\end{tabular} & \begin{tabular}[c]{@{}l@{}}7\\ 8\\ 9\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 3}
\label{table:qual3}
\end{table}
It seems that here the unigram approach fell short, whereas the bigram approach, trained on prioriting local method name after \textit{self}, has performed quite well. In the alphabetic approach, the other two method suggestions seem to be buried below the first 10, whereas \textit{readFile} gets a better position.

\item [Example 4:] \hfill
\begin{lstlisting}
    TokenProcessing >> replaceWithPlaceholders
        | data tokens tokenTypes arr |
        data := self returnProcessedData.
        to
\end{lstlisting}
Typing \textit{to} we are expecting to have \textit{tokens} and \textit{tokenTypes}, as those are temporary variables we have declared and we might want to initialise them with some value. Here is what we got suggested:
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & \begin{tabular}[c]{@{}l@{}}tokenTypes\\ tokens\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    bigram & \begin{tabular}[c]{@{}l@{}}token\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 4}
\label{table:qual4}
\end{table}
The order in the unigram and bigram cases is a bit better, as having a shorter word suggested first is preferable, but in general all three methods worked well in this one.

\item [Example 5:] \hfill
\begin{lstlisting}
    UnigramTableCreator ne
\end{lstlisting}
Typing this in the Playground, we expect \textit{new} and maybe even \textit{new:}, depending on the context.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}l@{}}2\\ 3\end{tabular} \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    bigram & \begin{tabular}[c]{@{}l@{}}new:\\ new\end{tabular} & \begin{tabular}[c]{@{}l@{}}4\\ 8\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 5}
\label{table:qual5}
\end{table}
The best positions are for the unigram sorter, but in general all the sorters suggested the desired options in the top 10.

\item [Example 6:] \hfill
\begin{lstlisting}
    in
\end{lstlisting}
In the Pharo IDE it is also possible to get completion suggestions even when creating a new method, and in this case we went to the instance side of a class and started typing \textit{in} as method name, wanting to get \textit{initialize}, as it is a very common method one might want to create in a class.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & - & - \\ \hline
    unigram & initialize & 2 \\ \hline
    bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 6}
\label{table:qual6}
\end{table}
The unigram sorter is the only that was able to suggest \textit{initialize} from only 2 characters typed in, and at the 2 position!

\item [Example 7:] \hfill
\begin{lstlisting}
    ExperimentalNgramTest >> testMostLikelyContinuations
    | expected actual word |
    word := 'lorem'.
    actual := model mostLikelyContinuations: word asNgram top: 3.
    expected := #('ipsum' 'dolor' 'lorem').
    self as
\end{lstlisting}
In test methods having an \textit{assert:equals:} call is quite common, and as there are a lot of tests being written, we are interested in having a completion suggestion that is as accurate as possible after \textit{as}.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & - & - \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}assert:\\ assert:equals:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    bigram & assert:equals: & 1 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 7}
\label{table:qual7}
\end{table}
Both the unigram and bigram sorters showed good results.

\item [Example 8:] \hfill
\begin{lstlisting}
    CompletionEvaluation >> averageSuccessRatio
    | sumS sumA |
	sumS := 0.
	sumA := 0.
	countSuccess do: [ :each | su ]
\end{lstlisting}
Here we are interested in completion inside the block, so our cursor is after \textit{su} before the bracket \textit{]}. Since inside the block we want to do assignment for the temporary variables, we are expecting \textit{sumA} and \textit{sumS} as suggestions.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & \begin{tabular}[c]{@{}l@{}}sumA\\ sumS\\ super\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    bigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 8}
\label{table:qual8}
\end{table}

\item [Example 9:] \hfill
\begin{lstlisting}
    RBScanner sc
\end{lstlisting}
In the Playground we want to get suggested method names on the class side of \textit{RBScanner} that begin with \textit{sc}. There are two: \textit{scanTokens:} and \textit{scanTokenObjects:}.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    bigram & \begin{tabular}[c]{@{}l@{}}scanTokens:\\ scanTokenObjects:\end{tabular} & \begin{tabular}[c]{@{}l@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 9}
\label{table:qual9}
\end{table}
On the one hand, all sorters showed essentially the same result; however, on the other hand, \textit{scanTokens:} is more likely to be used next to \textit{RBscanner} and it is a shorter token, so the bigram result is slightly superior.

\item [Example 10:] \hfill
\begin{lstlisting}
    UnigramTableCreator >> writeToFile
    | data stream |
    data := self frequencies.
    stream := TokenProcessing frequenciesFile writeStream.
    (NeoCSVWriter on: stream)
        nextPut: #(key value);
        ne
\end{lstlisting}
Now, for an even more creative example, in Pharo there can be multiple messages sent to a receiver, as long as they are all separated by a semicolon \textit{;}. Hence, having a cursor after \textit{ne}, we know we are sending a message to \textit{NeoCSVWriter on: stream} and want to get suitable suggestions, which are \textit{nextPut:} and \textit{nextPutAll:}.

In the table of results below we see that the unigram sorter got the best result.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \textbf{SORTER TYPE} & \textbf{COMPLETION SUGGESTION} & \textbf{POSITION} \\ \hline
    alphabetic & - & - \\ \hline
    unigram & \begin{tabular}[c]{@{}l@{}}nextPutAll:\\ nextPut:\end{tabular} & \begin{tabular}[c]{@{}l@{}}2\\ 3\end{tabular} \\ \hline
    bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation, example 10}
\label{table:qual10}
\end{table}
\end{description}

To briefly sum up the results of the manual qualitative evaluation, the unigram sorter is still the best one out of the three, the same as in the quantitative evaluation. However, as seen in these examples, the bigram sorter is not very far behind, also more often than not prioriting the desired completions, which disagrees with the quantitative evaluation. So, while based on cold data, the bigram sorter is the worst of the three \textit{on average}, from the manual evaluation it seems that it gives perfectly adequate results even for more 'tricky' examples.

\section{Summary}
\label{sec:Evaluation-Summary}
In short, the most relevant things you know about evaluation practices in general, as well as the evaluation results of our experiment in particular are the following:
\begin{itemize}
    \item quantitative (automatic) evaluation involves evaluating the model on some benchmarks, whereas the qualitative (manual) approach involves evaluating the results on the go, as the program is being normally executed
    \item there are some challenges when properly setting up the qualitative approach, as it involves a lot of resources; we didn't have them and so conducted a minimalist version of the approach
    \item based on the evaluation tool that we created and used to evaluate average success rate (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results) for each sorting strategy, the unigram sorter performed much better than the alphabetic sorter but, surprisingly, the bigram one performed worse
    \item for the qualitative evaluation, we picked 10 interesting completion cases (with no bias towards any strategy) and tested them in the real environment and recorded the positions of completion suggestions we were interested in; based on this, the unigram approach was once again the best, but this time the bigram approach followed not far behind, and the alphabetic was worse
    \item hence we can officially state that the unigram sorter is a clear improvement on the alphabetic approach, whereas the bigram sorter seems to lack the accuracy
\end{itemize}

In conclusion, the goal of the project has been reached: the completion in the Pharo IDE has indeed improved through using the N-gram language models for the task of sorting the results. However, contrary to the intuition, not only has the bigram approach performed worse than the unigram, it also doesn't seem to have performed better than even the alphabetic sorter. This yields more questions about the exact challenges of implementing this approach that haven't been met, and can be a good basis for future work in this topic.