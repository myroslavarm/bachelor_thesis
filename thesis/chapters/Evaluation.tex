\chapter{Evaluation}
\label{chap:Evaluation}

\epigraph{"It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong."}{--- \textup{Richard Feynman}}

In this chapter, we will go over different approaches and challenges for evaluating code completion, as well as analyse the results from the evaluation experiments for this project.

\section{Evaluation Overview}
\label{sec:Evaluation-Overview}
Evaluation is an essential part of any research process. In the Richard Feynman quote above, the evaluation \textit{is} the experiment he is referring to. To paraphrase and make it more relevant to software engineering, it does not matter what kind of tool you implemented and how well do you think it works, there need to be clear metrics the implementation is measured on and a clear definition of what is considered to be a successful evaluation.

When it comes to scientific research, there are two main types of evaluation one can perform: quantitative and qualitative.

Quantitative, as can be guessed from its name, refers to the type of an evaluation experiment that is based on quantifiable data and uses objective metrics. This is the predominant evaluation approach in the natural sciences and computer science, i.e. measuring physical parameters in the case of the former, or testing performance scores and accuracy in the latter.

Qualitative methods, more native to fields such as sociology or psychology, are now being increasingly used in evaluation of computer systems and software tooling\footnote{\cite{Hazz06a} in their paper discuss the qualitative approach and its relationship to the quantitative one.}. The main objective is to get the "feel" for the system and deeper explore the human interaction experience, with the data gathered primarily from observations and interviews and subsequently analysed.

Conventionally, a proper qualitative evaluation\footnote{\cite{Kapl05a} describe an extensive qualitative evaluation example and talk about the key benefits and considerations of this approach.} requires an extensive set-up, involving multiple people, recording their coding process, conducting feedback interviews, and so on. However, due to lack of time, the qualitative evaluation I conducted for this project is more minimal. Particularly, it involves manually selecting a number of common use cases and interacting with them as a user normally would, while recording the results and their meaning.

\section{Challenges in Evaluating Completion}
\label{sec:Evaluation-Challenges}
Coming up with a way to most accurately evaluate code completion is not trivial. One needs to have an idea how best to reproduce the process and what the metrics should be, as well as have an access to the resources needed. Mainly, here are the main challenges that need to be considered:
\begin{enumerate}
    \item Resources limitation. For benchmarking, there needs to be a large enough dataset to be able to properly evaluate the performance of the completion tool. It requires time to gather the data if it is not already available. If it already is and it is too big, then monetary and physical resouces come into play: one might then need better and faster hardware, more machines, and so on. Finally, one needs time and people to develop a suitable evaluation tool and set everything up well. For a qualitative evaluation, involving multiple people and managing time and money right are a concern, too. 
    \item Finding appropriate metrics. Ultimately, the goal of the evaluation is to be able to say whether the completion performs well or badly, and just \textit{how} well or \textit{how} badly. Choosing metrics which will adequately represent the quality of the performance takes a lot of careful analysis and research. Getting this part right considerably influences the validity of the end result of the evaluation.
    \item Reproducibility. Less a problem for a qualitative approach, where the idea is to let people use the tool as is and report the results, for the quantitative evaluation it is important to come up with a way to make code completion testable, or reproducible. This brings us back to the resources, as a suitable dataset is needed.
\end{enumerate}

Finally, this is all a matter of correctly managing pros and cons of the evaluation. There are multiple ways to perform it, and multiple reasons to do something one way or another. The knowledge, interests, perceptions and skills of the researches will undoubtedly play a role in the evaluation, and the subjective nature of the process is as undeniable part of the experiment and needs to be carefully addressed. In the end, the goal is to represent the process as closely as possible to how it really happens, and report the results that will be suitably meaningful, while exercising a healthy amount of unbiased professional detachment.

For this project both the quantitative and the qualitative approaches were used as the first provides concise numeric results of the completion sorting performance and the second allows to analyse common cases and demonstrates the actual human computer interaction as it would happen with regular users (developers using the Pharo IDE). The next two sections detail the experiments and their results.

\section{Quantitative Evaluation}
\label{sec:Evaluation-Quantitative}
\subsection{Experiment}
The quantitative evaluation approach performed for this project is based on the paper by \cite{Robb08a}, wherein they used the change history data to evaluate the performance of their completion.

The idea with this approach is to recreate the coding process as it would happen naturally, i.e. a gradual appearance of new code as if it is being entered at the moment, and test the completion during that. Just as if a developer is typing and invoking a code completion engine at every step, with this approach the completion gets triggered for every token with the cursor between the 2nd and the 8th position (the range taken from the \cite{Robb08a} paper), and we can compare whether the result suggested at that step matches the one that followed in real history.

For evaluation several Pharo classes were chosen at random, each containing multiple methods. For each of them I repeat this process of calling the completion at each step and perform the following:
\begin{itemize}
    \item check whether the correct completion is present in the list of suggestions
    \item if it is, the results are only considered successful if the correct completion is also present in the top-10 suggestions
    \item for those cases I record the current sequence (i.e. what has been "typed in" up to that point), prefix length, the actual correct match, and its position in the list of suggestions
    \item I also calculate the accuracy (average success rate) for all the methods evaluated (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results)
\end{itemize}

\subsection{Results}
The three models I am comparing are the unigram and bigram sorters that were implemented as part of this work, and the alphabetic sorter, which is the existing default sorter of the Pharo IDE completion. The alphabetic sorter, as can be guessed from its name, simply sorts the completion suggestions alphabetically. Here we use it as a comparison baseline, as the idea is to find out whether indeed the n-gram based sorting performs better and improves the relevancy of code completion suggestions, in comparison to the alphabetic sorter (and the unigram and the bigram strategies tested against each other).

Table~\ref{table:quan1} presents the accuracy (average success rate) for each of the classes and sorting strategies tested.

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
    \hline
    \textbf{Class} & \textbf{\# of Methods} & \textbf{Alphabetic} & \textbf{Unigram} & \textbf{Bigram} \\ \hline
    OrderedCollection & 64 & 0.32 & 0.38 & 0.30 \\ 
    CompletionEvaluation & 9 & 0.24 & 0.25 & 0.22 \\ 
    FrequencyCompletionSorter & 2 & 0.48 & 0.58 & 0.47 \\ 
    NgramModel & 21 & 0.32 & 0.36 & 0.29 \\ \hline
    \end{tabular}
\caption{Quantitative evaluation: the average success ratio (accuracy) for each class and sorting strategy}
\label{table:quan1}
\end{table}

From the results, we can see that the unigram sorter performs better than the alphabetic sorter, as expected. However, surprisingly, the bigram sorter performs both worse than the unigram sorter and even slightly worse than the alphabetic one.

\section{Qualitative Evaluation}
\label{sec:Evaluation-Qualitative}
\subsection{Experiment}
For this evaluation approach, 10 examples were manually preselected. The main idea for each of the examples was to test a completion case which would be commonly encountered by a user while coding in the Pharo IDE. These examples include completion of message sends, method names, variable names, expressions inside a block, expressions in parentheses, muiltiple message sends, and so on. To emphasise: each of those examples is of interest because they are indicative of a common coding process in Pharo, and that is the sole reason they were chosen: none of the examples were chosen with any bias towards the sorting strategies. 

\subsection{Results}
Note: if the \textit{completion suggestion} and \textit{position} columns contain '-' for any sorting strategy, it means that the completion suggestions expected for that code example were not in any of the top 10 positions of the suggestions list.

It is worth noting that all the results are only recorded after two symbols were typed in, and having correct (most relevant) suggestions with such a short prefix appear among the top 10 results can be considered best-case scenario (as the correct suggestions would definitely be more likely to be positioned higher with a longer prefix). Ergo, when the desired suggestion is missing from the top 10, it does not mean a complete failure of the completion engine or a sorting strategy used, but rather hints that there is more work required to get to the correct suggestions, such as scrolling or typing more characters. However, as making the results as effective as possible is the goal of improving code completion in the first place, consistent placement of relevant results in the top 10 even after two symbols is a considerable advantage and does demonstrate which sorting strategy performs better.

Below you can see the 10 examples tested, the results, and their explanations.

\begin{description}
\item [Example 1:] \hfill
\begin{lstlisting}
    col := OrderedCollection new.
    col ad
\end{lstlisting}
This was typed into the Playground and the results were recorded with the cursor directly after \textit{ad}. For this context, the completion suggestions a developer would expect would be \textit{add:} and \textit{addAll:}. Here are the actual results:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}add:\\ addAll:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: the results of completing a message send in the Playground}
\label{table:qual1}
\end{table}
As we can see in the Table~\ref{table:qual1}, both the alphabetic and the bigram sorters did not manage to provide any relevant results in the top 10 suggestions, whereas the result from the unigram turned out exactly as expected.

\item [Example 2:] \hfill
\begin{lstlisting}
    something := 1.
    (something = 1) if
\end{lstlisting}
Another example coded in the Playground, with the cursor at \textit{if}. The expected results would be \textit{ifTrue:}, \textit{ifTrue:ifFalse:}, and \textit{ifFalse:}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}ifTrue:\\ ifTrue:ifFalse:\\ ifFalse:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & ifTrue: & 5 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing a message send to an expression in parentheses}
\label{table:qual2}
\end{table}
In the Table~\ref{table:qual2} we can see that the alphabetic sorter once again did not suggest anything of interest, the unigram demonstrated an exemplary performance, and the bigram, while less than ideal, still suggested one of the correct options in the top 10 position (albeit a bit lower, which would require either typing more symbols or pressing the down arrow key to get to).

\item [Example 3:] \hfill
\begin{lstlisting}
    TokenProcessing >> returnProcessedData
        | tokensDataFrame |
        tokensDataFrame := self re
\end{lstlisting}
This was typed in the Editor, in the method \textit{returnProcessedData} of class \textit{TokenProcessing}. This is done to try out some more elaborate examples where the local context can play a role.

Here, as \textit{rejectInvalidTokens}, \textit{replaceWithPlaceholders} and \textit{readFile} are also methods on the instance side of the \textit{TokenProcessing} class, we as users would expect to get their names as suggestions after typing \textit{self}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & readFile & 3 \\ \hline
    Unigram & - & - \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}rejectInvalidTokens\\ replaceWithPlaceholders\\ readFile\end{tabular} & \begin{tabular}[c]{@{}r@{}}7\\ 8\\ 9\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing a message send after self, with awareness of local context}
\label{table:qual3}
\end{table}
Table~\ref{table:qual3}: it seems that the unigram approach fell short, whereas the bigram approach, trained on prioriting local method names after \textit{self}, has performed quite well. In the alphabetic approach, the other two method suggestions seem to be buried below the first 10, whereas \textit{readFile} gets a better position.

\item [Example 4:] \hfill
\begin{lstlisting}
    TokenProcessing >> replaceWithPlaceholders
        | data tokens tokenTypes arr |
        data := self returnProcessedData.
        to
\end{lstlisting}
By typing \textit{to} we are expecting to get \textit{tokens} and \textit{tokenTypes}, as those are temporary variables that we declared and now might want to initialise with some values. Here are the results:
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}tokenTypes\\ tokens\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}tokens\\ tokenTypes\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing temporary variable names, with awareness of local context}
\label{table:qual4}
\end{table}
Here (Table~\ref{table:qual4}) the order in the unigram and bigram cases is slightly better, as having a shorter word suggested first is preferable, but in general all three approaches worked quite well for this example.

\item [Example 5:] \hfill
\begin{lstlisting}
    UnigramTableCreator ne
\end{lstlisting}
Typing this in the Playground, we expect \textit{new} and maybe even \textit{new:}, depending on the context.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}new\\ new:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}new:\\ new\end{tabular} & \begin{tabular}[c]{@{}r@{}}4\\ 8\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing message send to a global variable (in this case, trying to create a new instance of a class)}
\label{table:qual5}
\end{table}
In the Table~\ref{table:qual5} we can see that the best positions are for the unigram sorter, but in general all the sorters suggested the desired options in the top 10.

\item [Example 6:] \hfill
\begin{lstlisting}
    in
\end{lstlisting}
In the Pharo IDE it is possible to get completion suggestions even when creating a new method, by typing in the desired method name in the code editor on the instance side. If it is a common method that users often create in their classes, having a completion suggestion is useful. As this is the case with the method \textit{initialize}, we would like to get the name suggested to us after typing in \textit{in}, as illustrated above.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & initialize & 2 \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing method name during its creation}
\label{table:qual6}
\end{table}
Table~\ref{table:qual6}: the unigram sorter is the only that was able to suggest \textit{initialize} from only two characters typed in, and at the 2nd position!

\item [Example 7:] \hfill
\begin{lstlisting}
    ExperimentalNgramTest >> testMostLikelyContinuations
    | expected actual word |
    word := 'lorem'.
    actual := model mostLikelyContinuations: word asNgram top: 3.
    expected := #('ipsum' 'dolor' 'lorem').
    self as
\end{lstlisting}
In test methods having an \textit{assert:equals:} call is quite common, and as there are a lot of tests being written, we would be interested in having a completion suggestion that is as accurate as possible after \textit{as}.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}assert:\\ assert:equals:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & assert:equals: & 1 \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing a message send to self, when self is a test class}
\label{table:qual7}
\end{table}
Table~\ref{table:qual7}: both the unigram and the bigram sorters showed good results, but not the alphabetic one.

\item [Example 8:] \hfill
\begin{lstlisting}
    CompletionEvaluation >> averageSuccessRatio
    | sumS sumA |
	sumS := 0.
	sumA := 0.
	countSuccess do: [ :each | su ]
\end{lstlisting}
Here we are interested to see how the sorting performs when we are inside a block, so the cursor is after \textit{su} before the bracket \textit{]}. Since inside the block we want to do an assignment for the temporary variables, we would be expecting \textit{sumA} and \textit{sumS} as suggestions.

\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}sumA\\ sumS\\ super\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}super\\ sumA\\ sumS\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\\ 3\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing inside a block}
\label{table:qual8}
\end{table}
The results in the Table~\ref{table:qual8} (above) are quite straightforward: all the sorters seem to perform well, although since one is less likely to want to call \textit{super} directly inside a block, the alphabetic one seems to be most relevant (purely coincidentally, as \textit{p} follows \textit{m}).

\item [Example 9:] \hfill
\begin{lstlisting}
    RBScanner sc
\end{lstlisting}
Typing the above sequence in the Playground, we want to get method names on the class side of \textit{RBScanner} that begin with \textit{sc}. There are two: \textit{scanTokens:} and \textit{scanTokenObjects:}, so they are the ones we should be suggested.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}scanTokenObjects:\\ scanTokens:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    Bigram & \begin{tabular}[c]{@{}l@{}}scanTokens:\\ scanTokenObjects:\end{tabular} & \begin{tabular}[c]{@{}r@{}}1\\ 2\end{tabular} \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing a message send in the Playground}
\label{table:qual9}
\end{table}
As can be seen in the Table~\ref{table:qual9}, all sorters showed essentially the same result. However, \textit{scanTokens:} is more likely to be used next to \textit{RBscanner} and it is a shorter token, so the bigram result is slightly superior.

\item [Example 10:] \hfill
\begin{lstlisting}
    UnigramTableCreator >> writeToFile
    | data stream |
    data := self frequencies.
    stream := TokenProcessing frequenciesFile writeStream.
    (NeoCSVWriter on: stream)
        nextPut: #(key value);
        ne
\end{lstlisting}
Now, for an even more creative example, in Pharo there can be multiple messages sent to a receiver, as long as they are all separated by a semicolon \textit{;}.

So having a cursor after \textit{ne}, we know we are sending a message to \textit{NeoCSVWriter on: stream} and want to get suitable suggestions, which are \textit{nextPut:} and \textit{nextPutAll:}.

In the Table~\ref{table:qual10} below we see that the unigram sorter got the best result.
\begin{table}[H]
    \centering
    \begin{tabular}{llr}
    \hline
    \textbf{Sorter Type} & \textbf{Completion Suggestion} & \textbf{Position} \\ \hline
    Alphabetic & - & - \\ \hline
    Unigram & \begin{tabular}[c]{@{}l@{}}nextPutAll:\\ nextPut:\end{tabular} & \begin{tabular}[c]{@{}r@{}}2\\ 3\end{tabular} \\ \hline
    Bigram & - & - \\ \hline
    \end{tabular}
\caption{Qualitative evaluation: completing one of multiple message sends, with awareness of local context}
\label{table:qual10}
\end{table}
\end{description}

To briefly sum up the results of the manual qualitative evaluation, the unigram sorter is still the best one out of the three, the same as in the quantitative evaluation. However, as seen in these examples, the bigram sorter is not very far behind, also more often than not prioriting the desired completions, which disagrees with the quantitative evaluation. So, while based on cold data, the bigram sorter is the worst of the three \textit{on average}, from the manual evaluation it seems that it gives perfectly adequate results even for more 'tricky' examples.

\section{Summary}
\label{sec:Evaluation-Summary}
\begin{itemize}
    \item Quantitative evaluation involves benchmarking the model and calculating performance scores, whereas the qualitative approach involves conducting the evaluation via letting people test the tool, observing the human computer interaction, recording feedback interviews, and so on.
    \item There are certain challenges in evaluating code completion, which can be divided into three main categories: resources limitation, finding appropriate metrics, and reproducibility.
    \item Based on the evaluation tool that I created and used to evaluate average success rate (i.e. out of all the attempts to complete the history, how many of them had the expected match in the first 10 results) for each sorting strategy, the unigram sorter performed better than the alphabetic sorter but, surprisingly, the bigram one performed worse.
    \item For the qualitative evaluation, 10 completion cases were preselected for their common occurrence when coding in the Pharo IDE, with no bias towards any sorting strategy. The examples were tested in the real environment, and the positions of the desired completion suggestions were recorded for each of them. Based on this, the unigram approach was once again the best, but this time the bigram approach followed not far behind, and the alphabetic was worse.
\end{itemize}