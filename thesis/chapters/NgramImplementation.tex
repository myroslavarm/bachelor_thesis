\chapter{Proposed solution}
\label{chap:Proposed solution}

\section{Background}
In our case, in code completion, we want to suggest the most likely tokens at every step. The main idea is, however, that we leave the existing, AST-based code completion engine in place, and just enhance the sorting strategy. That is, after we already have a list of completion candidates proposed to us, we sort it based on each ngram's probability and therefore show the most relevant completions first. In order to do that, we figured it would make sense to train our model on source code data, write it to file, and at every point fetch the relevant, pre-processed information from file where it is stored.

The source code history data used for this was retrieved by \cite{Zait20a} for their research regarding characterising Pharo code. In particular, the data comes from 50 projects, which consisted of 824 packages, 13,935 classes, and 151,717 methods. In particular, we used the dataset where the source code was already split into tokens and respective token types for each method. In this dataset, even delimiters and non-alphabetic tokens were included, and comments were considered tokens. For example, tokens could include '"Here is a comment for this method"' and '.', and their respective token types would be 'COM' and 'DOT'.

Due to a large number of tokens in our dataset, we needed to be mindful of potential time constraints, as the lookup of ngram probabilities used for sorting had to be fast enough to not make the developer pause and wait for it. Throughout the course of the experiment, we came up with various ways to additionally reduce the dataset, and each of those attempts will be described in the following sections.

\section{Unigram Sorting}
After additional cleaning of the data, we recorded individual token frequencies (i.e. number of occurrences in the source code history for each token) by counting and writing them to file. However, when using the results to sort code completion candidates, we faced a problem: as the number of tokens was around 150k, the lookup was slow enough to be noticeable and break the developer's flow when typing. 

Our approach to improve the time efficiency was to set a certain threshold for the number of occurences, and to cut off all the tokens with frequencies below it. We put the threshold equal to 10, meaning if the token occured less than 10 times throughout the whole history, it was irrelevant enough to be discarded. This way, we were able to cut off most of the miscellaneous, rarely encountered tokens, and shortened our dataset from 150k to just 16k entries. This made the frequencies lookup during completion sorting very fast and, as a result, it was now possible to type and use completion information without any delays.

\section{Bigram Sorting}
The bigram model trained on source code data was implemented with the help of the NgramModel library available in Pharo. In the process of working on implementing the model, we extended the existing library with the following functionality:
\begin{itemize}
    \item functionality to filter the table of ngrams (cut off ngram sequences with counts below a certain threshold)
    \item adding writing to and reading ngram models from file
\end{itemize}
The one-time tokens, such as custom strings and comments, were not as relevant to us during implementing the unigram sorting strategy, however the bigram model became "too heavy" due to many combinations of such tokens, and needed to be additionally reduced. Our solution was to go back to the data processing step and replace those "one-time" tokens with placeholders. In particular, we added placeholders for strings, comments, symbols, characters, arrays and numbers (such as all strings being written as <str> and all numbers as <num>). This allowed us to greatly reduce both the number of total tokens and subsequent ngram sequences, which helped both the bigram and the unigram model, as well as the lookup speed.

As a result, the bigram-based sorting can now be used by setting the default sorter as the BigramCompletionSorter. To make it work, we learnt how to infer the previous word (i.e. the "history" in bigrams) from the context of the method in which we are typing, as well as calculated the probability of each of the ngram sequences (history word + each completion candidate). The sorting of results is thus based on this probability of the generated bigram sequences, however only the completion candidates (i.e. words we are predicting in the bigram model) are the ones being sorted and displayed in the most relevant order.