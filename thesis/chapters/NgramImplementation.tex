\chapter{Proposed solution}
\label{chap:Proposed solution}

\section{Background}
In our case, in code completion, we want to suggest the most likely tokens at every step. The main idea is, however, that we leave the existing, AST-based code completion engine in place, and just enhance the sorting strategy. That is, after we already have a list of completion candidates proposed to us, we sort it based on each ngram's probability and therefore show the most relevant completions first. In order to do that, we figured it would make sense to train our model on source code data, write it to file, and at every point fetch the relevant, pre-processed information from file where it is stored.

The source code history data used for this was retrieved by \cite{Zait20a} for their research regarding characterising Pharo code. In particular, the data comes from 50 projects, which consisted of 824 packages, 13,935 classes, and 151,717 methods. In particular, we used the dataset where the source code was already split into tokens and respective token types for each method. In this dataset, even delimiters and non-alphabetic tokens were included, and comments were considered tokens. For example, tokens could include '"Here is a comment for this method"' and '.', and their respective token types would be 'COM' and 'DOT'.

Due to a large number of tokens in our dataset, we needed to be mindful of potential time constraints, as the lookup of ngram probabilities used for sorting had to be fast enough to not make the developer pause and wait for it. Throughout the course of the experiment, we came up with various ways to additionally reduce the dataset, and each of those attempts will be described in the following sections.

\section{Unigram Sorting}
After additional cleaning of the data, we recorded individual token frequencies (i.e. number of occurrences in the source code history for each token) by counting and writing them to file. However, when using the results to sort code completion candidates, we faced a problem: as the number of tokens was around 150k, the lookup was slow enough to be noticeable and break the developer's flow when typing. 

Our approach to improve the time efficiency was to set a certain threshold for the number of occurences, and to cut off all the tokens with frequencies below it. We put the threshold equal to 10, meaning if the token occured less than 10 times throughout the whole history, it was irrelevant enough to be discarded. This way, we were able to cut off most of the miscellaneous, rarely encountered tokens, and shortened our dataset from 150k to just 16k entries. This made the frequencies lookup during completion sorting very fast and, as a result, it was now possible to type and use completion information without any delays.

\section{Bigram Sorting}
\begin{itemize}
    \item implemented Bigram with the help of the NgramModel Pharo library, which in the process extended by: (1) adding the functionality to filter the table of ngrams (2) adding writing to and reading ngram models from file
    \item replaced "one-time" tokens with placeholders, in particular added placeholders for strings, comments, symbols, characters, arrays and numbers; which means that both the number of total tokens and subsequent ngram sequences has been greatly reduced even before further filtering
    \item implemented Bigram-based sorting which can now be used by setting the default sorter as the BigramCompletionSorter. to make it work needed to be able to get a previous word (history in bigrams) from context and calculate the probability of each of the ngram sequences (history word + each completion candidate), and sort based on that
\end{itemize}