\chapter{Introduction}
\label{chap:Introduction}

\section{Context}
Code completion, also often referred to as autocompletion \oz{code completion is a special case of autocompletion}, is one of the most used features in any IDE. \oz{It greatly improves speed and accuracy of typing and helps developers...} Whether it is being used for improving speed and accuracy of typing, or is used as an API guide, helping developers find their way around \oz{\st{native} Why native?} libraries, code completion is an essential part of any successful IDE \oz{\st{and a fair share of}} \oz{or} code editor\oz{\st{s}} \oz{you already said that}.

Code completion is \oz{\st{basically}} used as a major productivity tool, and is one of the first things a developer notices as soon as they start coding. The speed with which the results are suggested, as well as their accuracy is very important, and it is something that can certainly "make or break" the workflow of the developer. \oz{I don't see the connection here. It should be fast and accurate - OK, but maybe it's a solved problem. For example, I never complain about completion in Sublime or Atom. So what is there to research?} Therefore, \oz{\st{it is no wonder that}} improving code completion quality is \oz{\st{also} also?} a \oz{\st{somewhat}} popular research topic, where \oz{\st{software engineers} researchers and software engineers are not the same people (generally speaking)} try to figure out a way to make code completion as effective, as accurate \oz{What is the difference between effective and accurate?}, and as fast \oz{Also, what do you mean when you say "fast"? The time that it takes for the list of completions to appear on the screen or the time that it takes for developer to find a completion that he or she is looking for?} as possible.

There are many approaches to \oz{\st{handling the process of}} completing code. The classic strategies have been to use lexical models or to use semantic models. Lexical \oz{\st{in this case essentially}} means completing words, or tokens, as they are called in source code, based on the prefix \oz{What is a "prefix"?} the user is typing, only relying on the text itself. Semantic models \oz{\st{mean using}} \oz{use} code structure analysis to tailor completions (for instance, type or semantic role). \oz{(1) Where does this terminology come from? You should cite something here.} \oz{(2) I'm confused: Is lexical based ONLY on prefix? Because if yes, then how does it propose completions? It must have some external input. Does semantic completion not use the prefix? I don't understand the difference between those two strategies} \oz{(3) Why doesn't machine learning-based completion fall under the category of semantic models?}

In the latest years statistical and machine learning \oz{\st{(ML)}} models have also gained popularity. \oz{What is the difference between statistical and machine learning models?} For the most part when using \oz{\st{ML}} \oz{machine learning}, source code analysis \oz{What is source code analysis in this case?} is also often disregarded, and the results of simply training the model on the code base and inferring all the possible code dependencies are used instead. \oz{Are you talking about manual feature selection VS end-to-end learning?}

\section{Problem}
\label{sec:Introduction-Problem}
For example, in the last couple of years there has been a lot of work carried out to improve code completion quality in the Pharo IDE\footnote{the name Pharo refers both to the dynamically-typed programming language that is a dialect of Smalltalk, as well as the interactive IDE where this language is used; from now on when referring to the language itself we will simply say \textit{Pharo}, whereas when referring to the IDE we will say \textit{the Pharo IDE}}. \oz{References?}

Among the approaches taken is replacing the old completion engine with an abstract syntax tree (AST) based one, that makes more use of the semantic information, and, more recently, adding a heuristics approach. \oz{(1) Bad English} \oz{(2) What is "old" completion?} \oz{(3) What is AST-based completion?} \oz{(4) What is athe "heuristic approach"?} However, there are more potential improvements from which developers can only benefit, such as improving the relevance of suggestions, which should also allow us to reduce the time and effort the developer spends to go through the list of completions. \oz{Didn't all those approaches improve the relevance of suggestions?}

As mentioned above, the current main implementation of code completion in the Pharo IDE is based on parsing and analysing AST information of source code, where we are able to determine the structure of the code, get the semantic role of tokens, as well as infer the type information where possible. The objective being to only suggest completions that are correct in the given context.

Therefore, once we have a list of semantically \oz{are you sure that "semantically" is a good word here?} suitable completion candidates, they are sorted alphabetically \oz{No! Using AST does not imply alphabetic sorting. So it's not "therefore"}. Unfortunately, this way the process of finding the desired completion candidate can be a bit cumbersome: the correct suggestions can be buried much lower than expected, which would prompt the developer to either scroll through the list, or to type even more characters, taking up valuable time and effort. Now, we believe this approach can be greatly improved by taking into account the relevance of suggestions, by analysing their use throughout source code history, and sorting the results based on that.

\oz{OK, to sum up: the problem statement is "Pharo code completion uses alphabetic sorter, which is not very effective"... well, isn't it obvious? When you use alphabetic sorter, you get words sorted by alphabet - nothing more...}

\oz{Besides, if I'm not mistaking, it was you who introduced the alphabetic sorter}

\oz{I think that you should be comparing your completion engine to (1) the one that we had in Pharo 3 years ago and if possible (2) the ones that other IDEs and code editors are using}

\section{Our Approach in a Nutshell}
\label{sec:Introduction-Approach}
\oz{\st{Hence, i} I}n this work we \oz{\st{intend to}} study how code completion can be improved using statistical language models for sorting the results, on top of an already existing semantic completion. In particular, \oz{\st{the idea is to} we} use the N-gram language model, as it has been documented to be used for such a task (\cite{Hind12a}). And specifically, we \oz{\st{intend to}} implement it in the Pharo IDE and evaluate whether this approach indeed enhances the quality of code completion. In \oz{\st{short, over the course of}} this work we \oz{\st{intend to}} answer the following questions:
\begin{itemize}
    \item Can we improve code completion in the Pharo IDE by sorting candidate completions with an N-gram language model?
    \item Can we build a tool based on a trained N-gram model that would propose completion fast enough to be actually suitable for developer use? \oz{Do you answer this question in your work?}
    \item How can we best numerically evaluate the results of code completion produced by different completion strategies?
\end{itemize}

\section{Contributions}
The contributions of this work are as follows:
\begin{itemize}
    \item We develop and train N-gram (in particular, uni- (N=1) and bigram (N=2)) language models on the tokenised source code of a dynamically-typed programming language (Pharo)
    \item We implement the sorting functionality based on the results of the above-mentioned training in an open source IDE
    \item We test whether the combination of semantic-based completion and statistical-based sorting can indeed improve the performance of a code completion engine in an IDE and be effective enough to be used by real developers
    \item To our knowledge, this exact approach of separately considering these two parts is unique \oz{I don't understand, what are the two parts and what is unique about this approach?}, and in addition nothing of the sort has been implemented in other Smalltalk-specific IDEs
\end{itemize}

\section{Structure of the Thesis}
{\hypersetup{linkcolor=black}
\begin{description}
	\item [Chapter \ref{chap:Related Works}. \nameref{chap:Related Works}] \hfill \\
	Here we give a detailed overview of the research done around code completion in the last decade and a half. It includes both the solely software engineering approaches predating the use of machine learning for this task, as well as exclusively machine learning experiments of the recent years.
	\item [Chapter \ref{chap:Completion in Pharo}. \nameref{chap:Completion in Pharo}] \hfill \\
	In this chapter, we give a detailed overview of the implementation details behind the current completion engine in the Pharo IDE, challenges of code completion for a dynamically typed language, as well as describe the idea behind the sorter plugin.
	\item [Chapter \ref{chap:N-gram Background}. \nameref{chap:N-gram Background}] \hfill \\
    Here we provide a theoretical background needed to understand this work, which includes the introduction to the concept of language models in general and N-grams in particular, as well as examples and associated challenges.
    \item [Chapter \ref{chap:Proposed solution}. \nameref{chap:Proposed solution}] \hfill \\
	In this chapter, we take a more detailed look at the idea of our solution, as well as give an extended description of the experiment.
	\item [Chapter \ref{chap:Evaluation}. \nameref{chap:Evaluation}] \hfill \\
	In this chapter, we go into depth about evaluation techniques and challenges, as well as detail the evaluation approach taken and the results of this work.
\end{description}
}