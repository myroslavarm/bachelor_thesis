\chapter{RelatedWork}
\label{chap:RelatedWork}

\section{Software naturalness}
In the famous paper "On The Naturalness of Software", Hindle et al. \cite{Hind12a} compared source code
to natural languages. They claim that code is even more repetitive, predictable and full of
patterns than human languages. In the paper, they also argue that code can be modelled by
statistical language models, which can be used to support software engineers. Their approach
was based on capturing high-level statistical regularity at the n-gram level by taking n-1
previous tokens that are already entered into the text buffer, and attempting to guess the
next token. Using this model, it is possible to estimate the most probable sequences of tokens
and suggest the most relevant code completions to developers.

The work by Hindle et al. \cite{Hind12a} served as a sort of catalyst for the following research. For
instance, Tu et al. \cite{Tu14a} also learnt that code "has a high degree of localness, where identifiers
(e.g. variable names) are repeated often within close distance" \cite{Alla18a}. Thus, they applied a cache
mechanism that assigns higher probability to tokens that have been observed most recently. In the
more recent years, researches started applying deep learning models such as deep recurrent
neural networks. These models predict each token sequentially, but loosen the fixed context-size
assumption, instead representing the context using a distributed vector representation \cite{Alla18a}.

\section{Conclusions from further experiments}
Hellendoorn, Vincent J., et al. \cite{Hell19a} recorded the results of a case study of 15,000 completions
(completion events) for VisualStudio. One of the conclustions they've reached is that even though
RNNs often outperform n-gram models in typical natural language settings, n-gram models are
sometimes a better choice for modeling source code. For example, the deep learner is better at
core method invocations but loses on third-party library calls, whereas the n-gram model
naturally outperforms it on internal API calls but loses out on the other categories.

Proksch, et al. \cite{Prok15a} worked on an extensible inference engine for intelligent code completion
systems, called PatternBased Bayesian Network (PBN). Eclipse Code Recommenders6 project adapted
the PBN approach for their intelligent call completion. They've also tested (evaluating quality,
speed and model size) Best Matching Neighbour algorithm, using additional context information
for more precise recommendations, and applying clustering techniques to improve model sizes.
They've reached the following conclusions: showing the developer hundreds of recommendations
may be as ineffective as showing none; intelligent code completions better target the needs of
developers that are unfamiliar with an API; at about 1,000 object usages, i.e. every tripling
of the input size, they get a smaller increase in prediction quality.

\section{More Papers}
"Are Deep Neural Networks the Best Choice for Modeling Source Code?"
We introduce a dynamically updatable, nested scope, unlimited vocabulary count-based
N-gram model that significantly outperforms all existing token-level models, including
very powerful ones based on deep learning. Our work illustrates that traditional
approaches, with some careful engineering, can beat deep learning models.

"Code Completion with Neural Attention and Pointer Networks"
In this paper, we apply neural language models on the code completion task, and
develop an attention mechanism which exploits the parent-children information on
program’s AST. To deal with the OoV values in code completion, we propose a pointer
mixture network which learns to either generate a new value through an RNN component,
or copy an OoV value from local context through a pointer component. Experimental
results demonstrate the effectiveness of our approaches.

"NEURAL CODE COMPLETION"
However, existing approaches to intelligent code completion either rely on strong typing
(e.g., Visual Studio for C++), which limits their applicability to widely used dynamically
typed languages (e.g., JavaScript and Python), or are based on simple heuristics and term
frequency statistics which are often brittle and are relatively error-prone.
Raychev et al. (2014 -- Code completion with statistical language models) and White et al.
(2015 -- Toward deep learning software repositories) explore how to use recurrent neural networks
(RNNs) to facilitate the code completion task. However, these works only consider running
RNNs on top of a token sequence to build a probabilistic model. Although the input sequence
considered in Raychev et al. (2014) is produced from an abstract object, the structural
information contained in the abstract syntax tree is not directly leveraged by the RNN structure
in both of these two works. In contrast, we consider extending LSTM, a RNN structure,
to leverage the structural information directly for the code prediction task.

"Code Completion with Statistical Language Models"
In this paper we presented a new approach to code completion based on a novel combination
of program analysis with statistical language models, and implemented that approach in a tool
called SLANG. Given a massive codebase, using program analysis, SLANG ﬁrst extracts abstract
histories from the data. Then, these histories are fed to a language model such as an n-gram
model or recurrent neural network model, which treats the histories as sentences in a natural
language and learns probabilities for each sentence.

MYROSLAVA CONCLUSION: seems most of this research hasn't been actually implemented; these are artificial
studies to check how to improve ML use for code completion without any real implementation.
This needs to be checked for sure, but if this is indeed the case, we have an advantage with our
approach, because if we manage to implement it, we will be in the minority. But of course, the
research for IDEs (current usage) still needs to be explored.

"A Statistical Semantic Language Model for Source Code"
Eclipse [4] and IntelliJ IDEA [12, 11] support template-based completion for common
constructs/APIs (for/while, Iterator).
https://javascript.software.informer.com/download-javascript-code-completion-tool-for-eclipse-plugin/
and intellisense

"Learning from Examples to Improve Code Completion Systems"
Actually integrated their implementation (BMN based) into Eclipse, but it's 2009.