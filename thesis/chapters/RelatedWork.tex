\chapter{RelatedWork}
\label{chap:RelatedWork}

\section{Overview of papers}
1. When Code Completion Fails: Case Study on
Real-World Completions (\href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812116}{link})
\begin{itemize}
	\item case study of 15k completions (completion events) for VisualStudio
	\item synthetic benchmarks misrepresent real-world completions
	\item state-of-the-art code completion models, including structured recommenders, Recurrent Neural Networks, and dynamic n-gram models
    \item We outline promising factors and outstanding challenges; e.g., models that dynamically integrate local data fare much better than static ones, but intraproject queries remain the hardest category.
    \item Benchmarks should first and foremost aim to address completions following their frequency of use, which we characterize, showing for example that intra-project API completions are surprisingly relevant.
    \item Finally, real-world efficacy is increased precisely by focusing on the least accurate predictions in synthetic data: these are far more common and time-consuming in realworld data.
    \item A static vocabulary is estimated on the training data and all events seen less than 10 times are treated as a generic "unknown" token, to produce a vocabulary of 75,913.
    \item trained ngrams
    \item interested in understanding how the discrepancies between real and artificial completion data impact real code completion models and tools
    \item Approaches to perform code completions span a wide spectrum of techniques, but, virtually, all intelligent code completion models attempt to relate the context at the site of a required completion to a context that has been observed in some large training corpus.
    \item split into two categories: Structural Feature Selection (domain knowledge of source code) and Linguistic Models of Code (ML/statistical models)
    \item "naturalness" of source code, source code is somewhat like text and follows logical patterns/structure
    \item tried both n-grams and deep learning (RNN)... Even though RNNs often outperform n-gram models in typical natural language settings, we include both, because n-gram models are sometimes a better choice for modeling source code
    \item API recommendation tools that only recommend public APIs would thus fail to address over half of the real-world queries
    \item The deep learner is better at core method invocations but loses on third-party library calls; the n-gram model naturally outperforms it on internal API calls but loses out on the other categories.
\end{itemize}

2. Intelligent Code Completion with Bayesian Networks (\href{https://dl.acm.org/doi/pdf/10.1145/2744200}{link})
\begin{itemize}
    \item main work is an extensible inference engine for intelligent code completion systems, called PatternBased Bayesian Network (PBN) -- Eclipse Code Recommenders6 project adapted the PBN approach for their intelligent call completion
    \item Best Matching Neighbour algorithm -- We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. 
    \item Showing the developer hundreds of recommendations (e.g., the type Text in the SWT framework of Eclipse1 lists 168 methods and field declarations) may be as ineffective as showing none.
    \item Intelligent code completions better target the needs of developers that are unfamiliar with an API. The FrUiT tool [Bruch et al. 2006] and its successor using the BestMatching Neighbor (BMN) algorithm [Bruch et al. 2009], which resulted in Eclipse Code Recommenders2, are examples of intelligent code completion systems.
    \item two further important quality dimensions that need to be considered for code completion engines to effectively support developers: inference speed and model size.
    \item evaluating quality, speed and model size
    \item it is possible to see saturation effects starting at about 1,000 object usages, that is, every tripling of the input size leads to a smaller increase in prediction quality.
    \item The system proposed by Heinemann et al. (2012) recommends methods to a developer that are relevant in the class currently under development.
    \item This article showed that parameter call sites and the enclosing class context do not contribute much to prediction quality. However, this information could be crucial to find the right proposals for some types.
    \item For example, the machine learning algorithm could detect and remove outliers in the dataset to further improve prediction quality. 
\end{itemize}