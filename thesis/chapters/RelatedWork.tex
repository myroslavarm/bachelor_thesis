\chapter{RelatedWork}
\label{chap:RelatedWork}

\section{Overview of papers}
1. When Code Completion Fails: Case Study on
Real-World Completions (\href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812116}{link})
\begin{itemize}
	\item case study of 15k completions (completion events) for VisualStudio
	\item synthetic benchmarks misrepresent real-world completions
	\item state-of-the-art code completion models, including structured recommenders, Recurrent Neural Networks, and dynamic n-gram models
    \item We outline promising factors and outstanding challenges; e.g., models that dynamically integrate local data fare much better than static ones, but intraproject queries remain the hardest category.
    \item Benchmarks should first and foremost aim to address completions following their frequency of use, which we characterize, showing for example that intra-project API completions are surprisingly relevant.
    \item Finally, real-world efficacy is increased precisely by focusing on the least accurate predictions in synthetic data: these are far more common and time-consuming in realworld data.
    \item A static vocabulary is estimated on the training data and all events seen less than 10 times are treated as a generic "unknown" token, to produce a vocabulary of 75,913.
    \item trained ngrams
    \item interested in understanding how the discrepancies between real and artificial completion data impact real code completion models and tools
    \item Approaches to perform code completions span a wide spectrum of techniques, but, virtually, all intelligent code completion models attempt to relate the context at the site of a required completion to a context that has been observed in some large training corpus.
    \item split into two categories: Structural Feature Selection (domain knowledge of source code) and Linguistic Models of Code (ML/statistical models)
    \item "naturalness" of source code, source code is somewhat like text and follows logical patterns/structure
    \item tried both n-grams and deep learning (RNN)... Even though RNNs often outperform n-gram models in typical natural language settings, we include both, because n-gram models are sometimes a better choice for modeling source code
    \item API recommendation tools that only recommend public APIs would thus fail to address over half of the real-world queries
    \item The deep learner is better at core method invocations but loses on third-party library calls; the n-gram model naturally outperforms it on internal API calls but loses out on the other categories.
\end{itemize}