\chapter{N-gram Implementation}
\label{chap:N-gram Implementation}

In this chapter we briefly go over the theoretical background for the N-grams model, as well as describe in detail our approach and experiments with it.
\section{N-gram Language Models}
Language models are models that assign probabilities to sentences or sequences of words (\cite{Jura09a}). Among them, one of the most common ones is the N-gram model, which is widely used in natural language processing (NLP) and computational biology. N-grams, which use the sequences of \textit{n} words, most commonly 2 (bigrams) or 3 (trigrams), as well as 1 (unigrams), are able to predict the next item in the form of a \textit{(n-1)} Markov model. This means that our prediction is based on \textit{n-1} previous words or, more accurately in relation to source code, tokens.

For example, if we want to know how likely it is that \textit{Object} will be followed by \textit{new} and we use the notation that $P(t|h)$ is probability of a token \textit{t} given history \textit{h} (\cite{Jura09a}), we can present our bigram in the following way:
\begin{equation}
    P(new|Object)
\end{equation}
Thus, the idea is, we want to count how many times we encounter \textit{Object} and then how many times we see \textit{Object new}, and divide the latter by the former:
\begin{equation}
    \frac{\text{Object new}}{\text{Object}}
\end{equation}

\section{Unigram Sorting}
explain approach and results

\section{Bigram Sorting}
explain approach and results